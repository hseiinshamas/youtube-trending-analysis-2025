{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "420ae823",
   "metadata": {},
   "source": [
    "# üìö YouTube Trending Analysis ‚Äî Steps 1‚Äì15\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e865104d",
   "metadata": {},
   "source": [
    "**Author:** Hussein Shamas  \n",
    "**What:** Clean, reproduce and analyze the YouTube trending dataset. This notebook implements exactly tasks 1‚Üí15.\n",
    "\n",
    "**Problem:** We faced a problem where the CSV files are not properly decoded as UTF-8 and we were reading them with the default encoding (utf-8) but the file was saved in ISO-8859-1 (Latin-1) youTube video titles can be in any language, including Chinese, Russian, Japanese, emojis, etc\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790e0599",
   "metadata": {},
   "source": [
    "## üîß Setup ‚Äî imports and display options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c71f03f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import standard libraries\n",
    "import pandas as pd  # dataframes\n",
    "import numpy as np   # numeric helpers\n",
    "import glob          # file pattern matching\n",
    "import json          # read json files\n",
    "\n",
    "# pandas display settings for nicer output\n",
    "pd.set_option('display.max_columns', None)  # show all columns when printing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f802f92",
   "metadata": {},
   "source": [
    "## 1) Create a single dataframe with concatenation of all CSV files, adding a `country` column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c92eeb4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined shape: (375942, 17)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>trending_date</th>\n",
       "      <th>title</th>\n",
       "      <th>channel_title</th>\n",
       "      <th>category_id</th>\n",
       "      <th>publish_time</th>\n",
       "      <th>tags</th>\n",
       "      <th>views</th>\n",
       "      <th>likes</th>\n",
       "      <th>dislikes</th>\n",
       "      <th>comment_count</th>\n",
       "      <th>thumbnail_link</th>\n",
       "      <th>comments_disabled</th>\n",
       "      <th>ratings_disabled</th>\n",
       "      <th>video_error_or_removed</th>\n",
       "      <th>description</th>\n",
       "      <th>country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>n1WpP7iowLc</td>\n",
       "      <td>17.14.11</td>\n",
       "      <td>Eminem - Walk On Water (Audio) ft. Beyonc√©</td>\n",
       "      <td>EminemVEVO</td>\n",
       "      <td>10</td>\n",
       "      <td>2017-11-10T17:00:03.000Z</td>\n",
       "      <td>Eminem|\"Walk\"|\"On\"|\"Water\"|\"Aftermath/Shady/In...</td>\n",
       "      <td>17158579</td>\n",
       "      <td>787425</td>\n",
       "      <td>43420</td>\n",
       "      <td>125882</td>\n",
       "      <td>https://i.ytimg.com/vi/n1WpP7iowLc/default.jpg</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Eminem's new track Walk on Water ft. Beyonc√© i...</td>\n",
       "      <td>cs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0dBIkQ4Mz1M</td>\n",
       "      <td>17.14.11</td>\n",
       "      <td>PLUSH - Bad Unboxing Fan Mail</td>\n",
       "      <td>iDubbbzTV</td>\n",
       "      <td>23</td>\n",
       "      <td>2017-11-13T17:00:00.000Z</td>\n",
       "      <td>plush|\"bad unboxing\"|\"unboxing\"|\"fan mail\"|\"id...</td>\n",
       "      <td>1014651</td>\n",
       "      <td>127794</td>\n",
       "      <td>1688</td>\n",
       "      <td>13030</td>\n",
       "      <td>https://i.ytimg.com/vi/0dBIkQ4Mz1M/default.jpg</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>STill got a lot of packages. Probably will las...</td>\n",
       "      <td>cs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5qpjK5DgCt4</td>\n",
       "      <td>17.14.11</td>\n",
       "      <td>Racist Superman | Rudy Mancuso, King Bach &amp; Le...</td>\n",
       "      <td>Rudy Mancuso</td>\n",
       "      <td>23</td>\n",
       "      <td>2017-11-12T19:05:24.000Z</td>\n",
       "      <td>racist superman|\"rudy\"|\"mancuso\"|\"king\"|\"bach\"...</td>\n",
       "      <td>3191434</td>\n",
       "      <td>146035</td>\n",
       "      <td>5339</td>\n",
       "      <td>8181</td>\n",
       "      <td>https://i.ytimg.com/vi/5qpjK5DgCt4/default.jpg</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>WATCH MY PREVIOUS VIDEO ‚ñ∂ \\n\\nSUBSCRIBE ‚ñ∫ http...</td>\n",
       "      <td>cs</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      video_id trending_date  \\\n",
       "0  n1WpP7iowLc      17.14.11   \n",
       "1  0dBIkQ4Mz1M      17.14.11   \n",
       "2  5qpjK5DgCt4      17.14.11   \n",
       "\n",
       "                                               title channel_title  \\\n",
       "0         Eminem - Walk On Water (Audio) ft. Beyonc√©    EminemVEVO   \n",
       "1                      PLUSH - Bad Unboxing Fan Mail     iDubbbzTV   \n",
       "2  Racist Superman | Rudy Mancuso, King Bach & Le...  Rudy Mancuso   \n",
       "\n",
       "   category_id              publish_time  \\\n",
       "0           10  2017-11-10T17:00:03.000Z   \n",
       "1           23  2017-11-13T17:00:00.000Z   \n",
       "2           23  2017-11-12T19:05:24.000Z   \n",
       "\n",
       "                                                tags     views   likes  \\\n",
       "0  Eminem|\"Walk\"|\"On\"|\"Water\"|\"Aftermath/Shady/In...  17158579  787425   \n",
       "1  plush|\"bad unboxing\"|\"unboxing\"|\"fan mail\"|\"id...   1014651  127794   \n",
       "2  racist superman|\"rudy\"|\"mancuso\"|\"king\"|\"bach\"...   3191434  146035   \n",
       "\n",
       "   dislikes  comment_count                                  thumbnail_link  \\\n",
       "0     43420         125882  https://i.ytimg.com/vi/n1WpP7iowLc/default.jpg   \n",
       "1      1688          13030  https://i.ytimg.com/vi/0dBIkQ4Mz1M/default.jpg   \n",
       "2      5339           8181  https://i.ytimg.com/vi/5qpjK5DgCt4/default.jpg   \n",
       "\n",
       "   comments_disabled  ratings_disabled  video_error_or_removed  \\\n",
       "0              False             False                   False   \n",
       "1              False             False                   False   \n",
       "2              False             False                   False   \n",
       "\n",
       "                                         description country  \n",
       "0  Eminem's new track Walk on Water ft. Beyonc√© i...      cs  \n",
       "1  STill got a lot of packages. Probably will las...      cs  \n",
       "2  WATCH MY PREVIOUS VIDEO ‚ñ∂ \\n\\nSUBSCRIBE ‚ñ∫ http...      cs  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# List of CSV files\n",
    "csv_files = glob.glob('data/csv/*videos.csv')\n",
    "dfs = []\n",
    "\n",
    "for file in csv_files:\n",
    "    # Extract country code\n",
    "    country_code = file.split('/')[-1][:2]\n",
    "    \n",
    "    # Try UTF-8 first, fallback to latin1\n",
    "    try:\n",
    "        df = pd.read_csv(file, encoding='utf-8', low_memory=False)\n",
    "    except UnicodeDecodeError:\n",
    "        df = pd.read_csv(file, encoding='latin1', low_memory=False)\n",
    "    \n",
    "    # Add country column\n",
    "    df['country'] = country_code\n",
    "    \n",
    "    # Clean text columns\n",
    "    for col in ['title', 'channel_title', 'tags']:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].astype(str).str.strip()\n",
    "    \n",
    "    dfs.append(df)\n",
    "\n",
    "# Concatenate all DataFrames\n",
    "if dfs:\n",
    "    youtube = pd.concat(dfs, ignore_index=True)\n",
    "    print('Combined shape:', youtube.shape)\n",
    "    display(youtube.head(3))\n",
    "else:\n",
    "    print(\"No CSV files were successfully read.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4079549",
   "metadata": {},
   "source": [
    "## 2) Extract all videos that have no tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3c1b4ac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Videos without tags: 37698\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>trending_date</th>\n",
       "      <th>title</th>\n",
       "      <th>channel_title</th>\n",
       "      <th>category_id</th>\n",
       "      <th>publish_time</th>\n",
       "      <th>tags</th>\n",
       "      <th>views</th>\n",
       "      <th>likes</th>\n",
       "      <th>dislikes</th>\n",
       "      <th>comment_count</th>\n",
       "      <th>thumbnail_link</th>\n",
       "      <th>comments_disabled</th>\n",
       "      <th>ratings_disabled</th>\n",
       "      <th>video_error_or_removed</th>\n",
       "      <th>description</th>\n",
       "      <th>country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>JwboxqDylgg</td>\n",
       "      <td>17.14.11</td>\n",
       "      <td>Canada Soccer's Women's National Team v USA In...</td>\n",
       "      <td>Canada Soccer</td>\n",
       "      <td>17</td>\n",
       "      <td>2017-11-13T05:53:49.000Z</td>\n",
       "      <td>[none]</td>\n",
       "      <td>36311</td>\n",
       "      <td>277</td>\n",
       "      <td>28</td>\n",
       "      <td>13</td>\n",
       "      <td>https://i.ytimg.com/vi/JwboxqDylgg/default.jpg</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Canada Soccer's Women's National Team face riv...</td>\n",
       "      <td>cs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>9B-q8h31Bpk</td>\n",
       "      <td>17.14.11</td>\n",
       "      <td>John Oliver Tackles Louis C.K. And Donald Trum...</td>\n",
       "      <td>TV Shows</td>\n",
       "      <td>22</td>\n",
       "      <td>2017-11-13T04:49:26.000Z</td>\n",
       "      <td>[none]</td>\n",
       "      <td>106029</td>\n",
       "      <td>1270</td>\n",
       "      <td>101</td>\n",
       "      <td>181</td>\n",
       "      <td>https://i.ytimg.com/vi/9B-q8h31Bpk/default.jpg</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>John Oliver on News, Politics ...</td>\n",
       "      <td>cs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>1UE5Dq1rvUA</td>\n",
       "      <td>17.14.11</td>\n",
       "      <td>Taylor Swift Perform Ready For It - SNL</td>\n",
       "      <td>Ken Reactz</td>\n",
       "      <td>24</td>\n",
       "      <td>2017-11-12T05:18:02.000Z</td>\n",
       "      <td>[none]</td>\n",
       "      <td>320964</td>\n",
       "      <td>8069</td>\n",
       "      <td>285</td>\n",
       "      <td>717</td>\n",
       "      <td>https://i.ytimg.com/vi/1UE5Dq1rvUA/default.jpg</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Thanks for watching please subscribe and subsc...</td>\n",
       "      <td>cs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>pmJQ4KwliX4</td>\n",
       "      <td>17.14.11</td>\n",
       "      <td>LATEST Q POSTS: ROTHSCHILDS, HOUSE OF SAUD, lL...</td>\n",
       "      <td>James Munder</td>\n",
       "      <td>2</td>\n",
       "      <td>2017-11-12T21:25:40.000Z</td>\n",
       "      <td>[none]</td>\n",
       "      <td>116820</td>\n",
       "      <td>1503</td>\n",
       "      <td>139</td>\n",
       "      <td>1066</td>\n",
       "      <td>https://i.ytimg.com/vi/pmJQ4KwliX4/default.jpg</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>https://pastebin.ca/3930472\\n\\nSupport My Chan...</td>\n",
       "      <td>cs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>lHcXhBojpeQ</td>\n",
       "      <td>17.14.11</td>\n",
       "      <td>‰∏âÂ±ÜTVBË¶ñÂ∏ùÔºåÊããÊ£Ñ10Âπ¥ÈùíÊ¢ÖÁ´πÈ¶¨È´ÆÂ¶ªÔºåÁÇ∫Â®∂Â∞è‰∏âÈÇÑ‰∏çÊÉúËàáÊØçÁµï‰∫§ÔºÅ</td>\n",
       "      <td>ÊòéÊòüÁôæÊõâÁîü</td>\n",
       "      <td>22</td>\n",
       "      <td>2017-11-12T12:49:50.000Z</td>\n",
       "      <td>[none]</td>\n",
       "      <td>88061</td>\n",
       "      <td>47</td>\n",
       "      <td>58</td>\n",
       "      <td>17</td>\n",
       "      <td>https://i.ytimg.com/vi/lHcXhBojpeQ/default.jpg</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cs</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       video_id trending_date  \\\n",
       "41  JwboxqDylgg      17.14.11   \n",
       "58  9B-q8h31Bpk      17.14.11   \n",
       "78  1UE5Dq1rvUA      17.14.11   \n",
       "86  pmJQ4KwliX4      17.14.11   \n",
       "98  lHcXhBojpeQ      17.14.11   \n",
       "\n",
       "                                                title  channel_title  \\\n",
       "41  Canada Soccer's Women's National Team v USA In...  Canada Soccer   \n",
       "58  John Oliver Tackles Louis C.K. And Donald Trum...       TV Shows   \n",
       "78            Taylor Swift Perform Ready For It - SNL     Ken Reactz   \n",
       "86  LATEST Q POSTS: ROTHSCHILDS, HOUSE OF SAUD, lL...   James Munder   \n",
       "98                   ‰∏âÂ±ÜTVBË¶ñÂ∏ùÔºåÊããÊ£Ñ10Âπ¥ÈùíÊ¢ÖÁ´πÈ¶¨È´ÆÂ¶ªÔºåÁÇ∫Â®∂Â∞è‰∏âÈÇÑ‰∏çÊÉúËàáÊØçÁµï‰∫§ÔºÅ          ÊòéÊòüÁôæÊõâÁîü   \n",
       "\n",
       "    category_id              publish_time    tags   views  likes  dislikes  \\\n",
       "41           17  2017-11-13T05:53:49.000Z  [none]   36311    277        28   \n",
       "58           22  2017-11-13T04:49:26.000Z  [none]  106029   1270       101   \n",
       "78           24  2017-11-12T05:18:02.000Z  [none]  320964   8069       285   \n",
       "86            2  2017-11-12T21:25:40.000Z  [none]  116820   1503       139   \n",
       "98           22  2017-11-12T12:49:50.000Z  [none]   88061     47        58   \n",
       "\n",
       "    comment_count                                  thumbnail_link  \\\n",
       "41             13  https://i.ytimg.com/vi/JwboxqDylgg/default.jpg   \n",
       "58            181  https://i.ytimg.com/vi/9B-q8h31Bpk/default.jpg   \n",
       "78            717  https://i.ytimg.com/vi/1UE5Dq1rvUA/default.jpg   \n",
       "86           1066  https://i.ytimg.com/vi/pmJQ4KwliX4/default.jpg   \n",
       "98             17  https://i.ytimg.com/vi/lHcXhBojpeQ/default.jpg   \n",
       "\n",
       "    comments_disabled  ratings_disabled  video_error_or_removed  \\\n",
       "41              False             False                   False   \n",
       "58              False             False                   False   \n",
       "78              False             False                   False   \n",
       "86              False             False                   False   \n",
       "98              False             False                   False   \n",
       "\n",
       "                                          description country  \n",
       "41  Canada Soccer's Women's National Team face riv...      cs  \n",
       "58                  John Oliver on News, Politics ...      cs  \n",
       "78  Thanks for watching please subscribe and subsc...      cs  \n",
       "86  https://pastebin.ca/3930472\\n\\nSupport My Chan...      cs  \n",
       "98                                                NaN      cs  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# identify rows where tags is exactly the string '[none]' (common in this dataset)\n",
    "no_tag_videos = youtube[youtube['tags'] == '[none]']  # filter rows with no tags\n",
    "\n",
    "# print count and preview\n",
    "print('Videos without tags:', len(no_tag_videos))\n",
    "no_tag_videos.head(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc596fd",
   "metadata": {},
   "source": [
    "## 3) For each channel, determine the total number of views\n",
    "\n",
    "**Note:** we compute this using the *latest snapshot per video+country* to avoid double-counting the same video's views on multiple trending days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "74603ab7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>channel_title</th>\n",
       "      <th>views</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23022</th>\n",
       "      <td>T-Series</td>\n",
       "      <td>549514687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18398</th>\n",
       "      <td>Ozuna</td>\n",
       "      <td>496503490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17659</th>\n",
       "      <td>NickyJamTV</td>\n",
       "      <td>467892045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6674</th>\n",
       "      <td>DrakeVEVO</td>\n",
       "      <td>364312023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2778</th>\n",
       "      <td>Bad Bunny</td>\n",
       "      <td>357112379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8465</th>\n",
       "      <td>Flow La Movie</td>\n",
       "      <td>337621571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7032</th>\n",
       "      <td>Ed Sheeran</td>\n",
       "      <td>332325013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>369</th>\n",
       "      <td>5-Minute Crafts</td>\n",
       "      <td>311537217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26044</th>\n",
       "      <td>WWE</td>\n",
       "      <td>303770764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28297</th>\n",
       "      <td>ibighit</td>\n",
       "      <td>300206826</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         channel_title      views\n",
       "23022         T-Series  549514687\n",
       "18398            Ozuna  496503490\n",
       "17659       NickyJamTV  467892045\n",
       "6674         DrakeVEVO  364312023\n",
       "2778         Bad Bunny  357112379\n",
       "8465     Flow La Movie  337621571\n",
       "7032        Ed Sheeran  332325013\n",
       "369    5-Minute Crafts  311537217\n",
       "26044              WWE  303770764\n",
       "28297          ibighit  300206826"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parse trending_date into proper datetime to identify latest snapshots\n",
    "# Kaggle trending_date format is 'yy.dd.mm' (e.g. '17.14.11' -> 2017-11-14)\n",
    "youtube['trending_date_parsed'] = pd.to_datetime(youtube['trending_date'], format='%y.%d.%m', errors='coerce')\n",
    "\n",
    "# create 'latest_snapshot' by taking the last row per (video_id, country) ordered by trending_date_parsed\n",
    "youtube_sorted = youtube.sort_values(['video_id', 'country', 'trending_date_parsed'])  # sort values so last is latest\n",
    "latest_snapshot = youtube_sorted.groupby(['video_id', 'country'], as_index=False).last()  # keep last metrics per video+country\n",
    "\n",
    "# Now compute total views per channel using the latest snapshot to avoid duplicates across days\n",
    "channel_views = latest_snapshot.groupby('channel_title', as_index=False)['views'].sum().sort_values('views', ascending=False)\n",
    "channel_views.head(10)\n",
    "\n",
    "#(If you just sum views from all rows, you get:\n",
    "#1000 + 1200 + 1500 = 3700 views.\n",
    "#But the video really has only 1500 views at the end, not 3700) thats why we compute the latest snapshot. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d1a33a",
   "metadata": {},
   "source": [
    "## 4) Save rows with disabled comments/ratings or error into `excluded`, and remove them from the original dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c5f40654",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Excluded shape: (13657, 18)\n",
      "Remaining after removal: (362285, 18)\n"
     ]
    }
   ],
   "source": [
    "# create excluded DataFrame with any of the problematic flags set\n",
    "excluded = youtube[\n",
    "    (youtube['comments_disabled'] == True) |\n",
    "    (youtube['ratings_disabled'] == True) |\n",
    "    (youtube['video_error_or_removed'] == True)\n",
    "].copy()  # copy to avoid SettingWithCopy warnings\n",
    "\n",
    "# show how many excluded\n",
    "print('Excluded shape:', excluded.shape)\n",
    "\n",
    "# remove those rows from youtube (operate on the full time-series youtube)\n",
    "youtube = youtube.drop(excluded.index).reset_index(drop=True)\n",
    "print('Remaining after removal:', youtube.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e091a396",
   "metadata": {},
   "source": [
    "## 5) Add a `like_ratio` column storing the ratio between likes and dislikes\n",
    "\n",
    "We handle divide-by-zero safely by setting ratio to NaN when dislikes is zero or missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "18b6689c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>likes</th>\n",
       "      <th>dislikes</th>\n",
       "      <th>like_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>787425</td>\n",
       "      <td>43420</td>\n",
       "      <td>18.135076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>127794</td>\n",
       "      <td>1688</td>\n",
       "      <td>75.707346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>146035</td>\n",
       "      <td>5339</td>\n",
       "      <td>27.352500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>132239</td>\n",
       "      <td>1989</td>\n",
       "      <td>66.485168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1634130</td>\n",
       "      <td>21082</td>\n",
       "      <td>77.513044</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     likes  dislikes  like_ratio\n",
       "0   787425     43420   18.135076\n",
       "1   127794      1688   75.707346\n",
       "2   146035      5339   27.352500\n",
       "3   132239      1989   66.485168\n",
       "4  1634130     21082   77.513044"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert likes/dislikes to numeric (in case they were strings)\n",
    "youtube['likes'] = pd.to_numeric(youtube['likes'], errors='coerce')\n",
    "youtube['dislikes'] = pd.to_numeric(youtube['dislikes'], errors='coerce')\n",
    "\n",
    "# compute ratio likes/dislikes safely\n",
    "youtube['like_ratio'] = youtube['likes'] / youtube['dislikes'].replace(0, np.nan)  # replace 0 with NaN to avoid Inf\n",
    "\n",
    "# show sample\n",
    "youtube[['likes','dislikes','like_ratio']].head(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a162aa5",
   "metadata": {},
   "source": [
    "## 6) Cluster the publish time into 10-minute intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fc81b389",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>publish_time</th>\n",
       "      <th>publish_time_parsed</th>\n",
       "      <th>time_interval</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-11-10T17:00:03.000Z</td>\n",
       "      <td>2017-11-10 17:00:03+00:00</td>\n",
       "      <td>2017-11-10 17:00:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-11-13T17:00:00.000Z</td>\n",
       "      <td>2017-11-13 17:00:00+00:00</td>\n",
       "      <td>2017-11-13 17:00:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-11-12T19:05:24.000Z</td>\n",
       "      <td>2017-11-12 19:05:24+00:00</td>\n",
       "      <td>2017-11-12 19:00:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-11-12T18:01:41.000Z</td>\n",
       "      <td>2017-11-12 18:01:41+00:00</td>\n",
       "      <td>2017-11-12 18:00:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-11-09T11:04:14.000Z</td>\n",
       "      <td>2017-11-09 11:04:14+00:00</td>\n",
       "      <td>2017-11-09 11:00:00+00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               publish_time       publish_time_parsed  \\\n",
       "0  2017-11-10T17:00:03.000Z 2017-11-10 17:00:03+00:00   \n",
       "1  2017-11-13T17:00:00.000Z 2017-11-13 17:00:00+00:00   \n",
       "2  2017-11-12T19:05:24.000Z 2017-11-12 19:05:24+00:00   \n",
       "3  2017-11-12T18:01:41.000Z 2017-11-12 18:01:41+00:00   \n",
       "4  2017-11-09T11:04:14.000Z 2017-11-09 11:04:14+00:00   \n",
       "\n",
       "              time_interval  \n",
       "0 2017-11-10 17:00:00+00:00  \n",
       "1 2017-11-13 17:00:00+00:00  \n",
       "2 2017-11-12 19:00:00+00:00  \n",
       "3 2017-11-12 18:00:00+00:00  \n",
       "4 2017-11-09 11:00:00+00:00  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# parse publish_time into timezone-aware datetime\n",
    "youtube['publish_time_parsed'] = pd.to_datetime(youtube['publish_time'], errors='coerce', utc=True)  # parse publish_time\n",
    "\n",
    "# floor to nearest 10 minutes to create time_interval\n",
    "youtube['time_interval'] = youtube['publish_time_parsed'].dt.floor('10min')  # group times into 10-minute bins\n",
    "\n",
    "youtube[['publish_time','publish_time_parsed','time_interval']].head(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7325c846",
   "metadata": {},
   "source": [
    "## 7) For each interval: number of videos, average likes/dislikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "723ac350",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time_interval</th>\n",
       "      <th>videos_count</th>\n",
       "      <th>avg_likes</th>\n",
       "      <th>avg_dislikes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2006-07-23 08:20:00+00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>459.000000</td>\n",
       "      <td>152.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2007-03-05 16:20:00+00:00</td>\n",
       "      <td>9</td>\n",
       "      <td>336.666667</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2007-06-25 06:50:00+00:00</td>\n",
       "      <td>12</td>\n",
       "      <td>579.833333</td>\n",
       "      <td>11.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2007-12-03 20:50:00+00:00</td>\n",
       "      <td>16</td>\n",
       "      <td>187.937500</td>\n",
       "      <td>15.687500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2008-01-07 21:20:00+00:00</td>\n",
       "      <td>10</td>\n",
       "      <td>99.900000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2008-01-13 01:30:00+00:00</td>\n",
       "      <td>2</td>\n",
       "      <td>1417.000000</td>\n",
       "      <td>49.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2008-02-12 20:20:00+00:00</td>\n",
       "      <td>3</td>\n",
       "      <td>1985.666667</td>\n",
       "      <td>124.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2008-04-05 18:20:00+00:00</td>\n",
       "      <td>4</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              time_interval  videos_count    avg_likes  avg_dislikes\n",
       "0 2006-07-23 08:20:00+00:00             1   459.000000    152.000000\n",
       "1 2007-03-05 16:20:00+00:00             9   336.666667      2.000000\n",
       "2 2007-06-25 06:50:00+00:00            12   579.833333     11.500000\n",
       "3 2007-12-03 20:50:00+00:00            16   187.937500     15.687500\n",
       "4 2008-01-07 21:20:00+00:00            10    99.900000      2.000000\n",
       "5 2008-01-13 01:30:00+00:00             2  1417.000000     49.500000\n",
       "6 2008-02-12 20:20:00+00:00             3  1985.666667    124.666667\n",
       "7 2008-04-05 18:20:00+00:00             4    46.000000      6.000000"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# group by the 10-minute time_interval and compute counts and averages\n",
    "interval_stats = youtube.groupby('time_interval').agg(\n",
    "    videos_count=('video_id','count'),  # how many rows/videos in that time interval\n",
    "    avg_likes=('likes','mean'),         # average likes in that interval\n",
    "    avg_dislikes=('dislikes','mean')    # average dislikes in that interval\n",
    ").reset_index()\n",
    "\n",
    "interval_stats.head(8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee68da6e",
   "metadata": {},
   "source": [
    "## 8) For each tag, determine the number of videos\n",
    "\n",
    "Note: `tags` is a string of multiple tags separated by `|`. We'll treat `[none]` as no tag and remove it from counting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "61c08395",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tags_split</th>\n",
       "      <th>video_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11273</th>\n",
       "      <td>\"2018\"</td>\n",
       "      <td>5337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>336397</th>\n",
       "      <td>\"funny\"</td>\n",
       "      <td>4063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277196</th>\n",
       "      <td>\"comedy\"</td>\n",
       "      <td>3029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>444104</th>\n",
       "      <td>\"news\"</td>\n",
       "      <td>2631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10481</th>\n",
       "      <td>\"2017\"</td>\n",
       "      <td>2496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>561479</th>\n",
       "      <td>\"video\"</td>\n",
       "      <td>1990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>511256</th>\n",
       "      <td>\"show\"</td>\n",
       "      <td>1824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>535454</th>\n",
       "      <td>\"television\"</td>\n",
       "      <td>1690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>551740</th>\n",
       "      <td>\"tv\"</td>\n",
       "      <td>1519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>436200</th>\n",
       "      <td>\"music\"</td>\n",
       "      <td>1446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>707597</th>\n",
       "      <td>\"√ë¬é√ê¬º√ê¬æ√ë¬Ä\"</td>\n",
       "      <td>1346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363537</th>\n",
       "      <td>\"humour\"</td>\n",
       "      <td>1332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363479</th>\n",
       "      <td>\"humor\"</td>\n",
       "      <td>1312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564511</th>\n",
       "      <td>\"vlog\"</td>\n",
       "      <td>1304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>490338</th>\n",
       "      <td>\"review\"</td>\n",
       "      <td>1288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>662535</th>\n",
       "      <td>\"√ê¬Ω√ê¬æ√ê¬≤√ê¬æ√ë¬Å√ë¬Ç√ê¬∏\"</td>\n",
       "      <td>1272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188736</th>\n",
       "      <td>\"TV\"</td>\n",
       "      <td>1269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>664065</th>\n",
       "      <td>\"√ê¬æ√ê¬±√ê¬∑√ê¬æ√ë¬Ä\"</td>\n",
       "      <td>1251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>672092</th>\n",
       "      <td>\"√ê¬ø√ê¬æ√ê¬ª√ê¬∏√ë¬Ç√ê¬∏√ê¬∫√ê¬∞\"</td>\n",
       "      <td>1248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>508013</th>\n",
       "      <td>\"serial\"</td>\n",
       "      <td>1204</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                tags_split  video_count\n",
       "11273               \"2018\"         5337\n",
       "336397             \"funny\"         4063\n",
       "277196            \"comedy\"         3029\n",
       "444104              \"news\"         2631\n",
       "10481               \"2017\"         2496\n",
       "561479             \"video\"         1990\n",
       "511256              \"show\"         1824\n",
       "535454        \"television\"         1690\n",
       "551740                \"tv\"         1519\n",
       "436200             \"music\"         1446\n",
       "707597          \"√ë¬é√ê¬º√ê¬æ√ë¬Ä\"         1346\n",
       "363537            \"humour\"         1332\n",
       "363479             \"humor\"         1312\n",
       "564511              \"vlog\"         1304\n",
       "490338            \"review\"         1288\n",
       "662535    \"√ê¬Ω√ê¬æ√ê¬≤√ê¬æ√ë¬Å√ë¬Ç√ê¬∏\"         1272\n",
       "188736                \"TV\"         1269\n",
       "664065        \"√ê¬æ√ê¬±√ê¬∑√ê¬æ√ë¬Ä\"         1251\n",
       "672092  \"√ê¬ø√ê¬æ√ê¬ª√ê¬∏√ë¬Ç√ê¬∏√ê¬∫√ê¬∞\"         1248\n",
       "508013            \"serial\"         1204"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create tags_split by splitting the tags string\n",
    "youtube['tags_split'] = youtube['tags'].astype(str).str.split('|')  # split into lists\n",
    "\n",
    "# explode so each tag has its own row\n",
    "tags_exploded = youtube.explode('tags_split')  # each tag becomes its own row\n",
    "\n",
    "# normalize tag text and remove [none] or empty\n",
    "tags_exploded['tags_split'] = tags_exploded['tags_split'].fillna('').astype(str).str.strip()  # strip spaces\n",
    "tags_exploded = tags_exploded[tags_exploded['tags_split'] != '']  # remove empty strings\n",
    "tags_exploded = tags_exploded[tags_exploded['tags_split'] != '[none]']  # remove [none]\n",
    "\n",
    "# count unique videos per tag (use video_id to avoid counting duplicates across trending days)\n",
    "tag_video_counts = tags_exploded.groupby('tags_split')['video_id'].nunique().reset_index(name='video_count').sort_values('video_count', ascending=False)\n",
    "\n",
    "tag_video_counts.head(20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a549970c",
   "metadata": {},
   "source": [
    "## 9) Find the tags with the largest number of videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "23118164",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tags_split</th>\n",
       "      <th>video_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11174</th>\n",
       "      <td>\"2018\"</td>\n",
       "      <td>5337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>335195</th>\n",
       "      <td>\"funny\"</td>\n",
       "      <td>4063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276159</th>\n",
       "      <td>\"comedy\"</td>\n",
       "      <td>3029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>442705</th>\n",
       "      <td>\"news\"</td>\n",
       "      <td>2631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10390</th>\n",
       "      <td>\"2017\"</td>\n",
       "      <td>2496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>559769</th>\n",
       "      <td>\"video\"</td>\n",
       "      <td>1990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>509729</th>\n",
       "      <td>\"show\"</td>\n",
       "      <td>1824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>533843</th>\n",
       "      <td>\"television\"</td>\n",
       "      <td>1690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>550052</th>\n",
       "      <td>\"tv\"</td>\n",
       "      <td>1519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>434829</th>\n",
       "      <td>\"music\"</td>\n",
       "      <td>1446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>707967</th>\n",
       "      <td>\"√ë¬é√ê¬º√ê¬æ√ë¬Ä\"</td>\n",
       "      <td>1371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362271</th>\n",
       "      <td>\"humour\"</td>\n",
       "      <td>1332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362213</th>\n",
       "      <td>\"humor\"</td>\n",
       "      <td>1312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>562796</th>\n",
       "      <td>\"vlog\"</td>\n",
       "      <td>1304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>488855</th>\n",
       "      <td>\"review\"</td>\n",
       "      <td>1288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>672298</th>\n",
       "      <td>\"√ê¬ø√ê¬æ√ê¬ª√ê¬∏√ë¬Ç√ê¬∏√ê¬∫√ê¬∞\"</td>\n",
       "      <td>1286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>662703</th>\n",
       "      <td>\"√ê¬Ω√ê¬æ√ê¬≤√ê¬æ√ë¬Å√ë¬Ç√ê¬∏\"</td>\n",
       "      <td>1279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187930</th>\n",
       "      <td>\"TV\"</td>\n",
       "      <td>1269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>664237</th>\n",
       "      <td>\"√ê¬æ√ê¬±√ê¬∑√ê¬æ√ë¬Ä\"</td>\n",
       "      <td>1251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>506488</th>\n",
       "      <td>\"serial\"</td>\n",
       "      <td>1204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>576346</th>\n",
       "      <td>\"youtube\"</td>\n",
       "      <td>1195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>405356</th>\n",
       "      <td>\"live\"</td>\n",
       "      <td>1190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299792</th>\n",
       "      <td>\"diy\"</td>\n",
       "      <td>1128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>330213</th>\n",
       "      <td>\"football\"</td>\n",
       "      <td>1110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>369951</th>\n",
       "      <td>\"interview\"</td>\n",
       "      <td>1110</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                tags_split  video_count\n",
       "11174               \"2018\"         5337\n",
       "335195             \"funny\"         4063\n",
       "276159            \"comedy\"         3029\n",
       "442705              \"news\"         2631\n",
       "10390               \"2017\"         2496\n",
       "559769             \"video\"         1990\n",
       "509729              \"show\"         1824\n",
       "533843        \"television\"         1690\n",
       "550052                \"tv\"         1519\n",
       "434829             \"music\"         1446\n",
       "707967          \"√ë¬é√ê¬º√ê¬æ√ë¬Ä\"         1371\n",
       "362271            \"humour\"         1332\n",
       "362213             \"humor\"         1312\n",
       "562796              \"vlog\"         1304\n",
       "488855            \"review\"         1288\n",
       "672298  \"√ê¬ø√ê¬æ√ê¬ª√ê¬∏√ë¬Ç√ê¬∏√ê¬∫√ê¬∞\"         1286\n",
       "662703    \"√ê¬Ω√ê¬æ√ê¬≤√ê¬æ√ë¬Å√ë¬Ç√ê¬∏\"         1279\n",
       "187930                \"TV\"         1269\n",
       "664237        \"√ê¬æ√ê¬±√ê¬∑√ê¬æ√ë¬Ä\"         1251\n",
       "506488            \"serial\"         1204\n",
       "576346           \"youtube\"         1195\n",
       "405356              \"live\"         1190\n",
       "299792               \"diy\"         1128\n",
       "330213          \"football\"         1110\n",
       "369951         \"interview\"         1110"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# top tags by number of unique videos\n",
    "top_tags = tag_video_counts.head(25)  # top 25 tags\n",
    "top_tags\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650616cd",
   "metadata": {},
   "source": [
    "## 10) For each (tag, country) pair, compute average ratio likes/dislikes\n",
    "\n",
    "We use `like_ratio` computed earlier. We compute mean per (tag, country) pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52eb5b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use tags_exploded (where each row is a tag instance) and compute average like_ratio per tag-country\n",
    "tag_country = tags_exploded.copy()  # work on a copy\n",
    "\n",
    "# remove rows where like_ratio is NaN (no dislikes info)\n",
    "tag_country = tag_country[tag_country['like_ratio'].notna()]\n",
    "\n",
    "# compute mean like_ratio per (tag, country)\n",
    "tag_country_ratio = tag_country.groupby(['tags_split','country'], as_index=False)['like_ratio'].mean().sort_values(['tags_split','country'])\n",
    "\n",
    "tag_country_ratio.head(20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee1d938",
   "metadata": {},
   "source": [
    "## 11) For each (trending_date, country) pair, the video with the largest number of views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfabcaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure trending_date_parsed exists (we parsed earlier)\n",
    "# for grouping by original trending_date string we can use trending_date_parsed or trending_date\n",
    "\n",
    "# find index of max views per (trending_date, country) on the time-series youtube\n",
    "idx = youtube.groupby(['trending_date','country'])['views'].idxmax()  # index of row with max views in each group\n",
    "top_video_per_day = youtube.loc[idx].reset_index(drop=True)  # select those rows\n",
    "\n",
    "top_video_per_day[['trending_date','country','video_id','title','views']].head(15)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe19b46",
   "metadata": {},
   "source": [
    "## 12) Divide `trending_date` into three columns: year, month, day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d889d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trending_date format is 'yy.dd.mm' (string). We'll split into three columns\n",
    "youtube['trending_date'] = youtube['trending_date'].astype(str)  # ensure string type\n",
    "youtube[['td_year','td_day','td_month']] = youtube['trending_date'].str.split('.', expand=True)  # split into 3 parts\n",
    "\n",
    "# convert to numeric and sensible names\n",
    "youtube['td_year'] = '20' + youtube['td_year'].astype(str)  # make full year e.g. '17' -> '2017'\n",
    "youtube[['td_year','td_month','td_day']].head(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40247c63",
   "metadata": {},
   "source": [
    "## 13) For each (month, country) pair, the video with the largest number of views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0149aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use td_month column we created and country to group\n",
    "idx_month = youtube.groupby(['td_month','country'])['views'].idxmax()  # index of max views per month-country pair\n",
    "top_video_per_month = youtube.loc[idx_month].reset_index(drop=True)\n",
    "\n",
    "top_video_per_month[['td_month','country','video_id','title','views']].head(15)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68416dac",
   "metadata": {},
   "source": [
    "## 14) Read all JSON files with the video categories (one JSON per country)\n",
    "\n",
    "We'll build a DataFrame `categories_df` with columns: country, id, title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08d9d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find category JSON files\n",
    "json_files = glob.glob('data/json/*category_id.json')\n",
    "\n",
    "categories = []  # list to store mappings\n",
    "\n",
    "for file in json_files:\n",
    "    country = file.split('/')[-1][:2]  # extract country code from filename\n",
    "     with open(file, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    items = data.get('items', [])\n",
    "    for item in items:\n",
    "        categories.append({'country': country, 'id': str(item['id']), 'title': item['snippet']['title']})\n",
    "\n",
    "categories_df = pd.DataFrame(categories)  # create dataframe of categories\n",
    "categories_df.head(12)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04937f25",
   "metadata": {},
   "source": [
    "## 15) For each country, determine how many videos have a category that is not assignable (i.e., category id not present in the json file for that country)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba44d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure category_id is string for safe merge\n",
    "youtube['category_id'] = youtube['category_id'].astype(str)\n",
    "categories_df['id'] = categories_df['id'].astype(str)\n",
    "\n",
    "# merge youtube with categories on (category_id, country) -> left merge so all youtube rows are kept\n",
    "merged = youtube.merge(categories_df, left_on=['category_id','country'], right_on=['id','country'], how='left', indicator=True)\n",
    "\n",
    "# rows where the merge did not find a category will have _merge == 'left_only'\n",
    "unassignable = merged[merged['_merge'] == 'left_only']\n",
    "\n",
    " unassigned_count = unassignable.groupby('country')['video_id'].nunique().reset_index(name='unassignable_count')\n",
    "\n",
    "unassigned_count\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb21548",
   "metadata": {},
   "source": [
    "## ‚úÖ Save final datasets and wrap up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdac806e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save cleaned full youtube dataframe and exploded tags for later use\n",
    "youtube.to_csv('final_youtube_full_steps1-15.csv', index=False, encoding='utf-8')\n",
    "tags_exploded.to_csv('final_youtube_tags_exploded_steps1-15.csv', index=False, encoding='utf-8')\n",
    "\n",
    "print('Saved: final_youtube_full_steps1-15.csv')\n",
    "print('Saved: final_youtube_tags_exploded_steps1-15.csv')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
